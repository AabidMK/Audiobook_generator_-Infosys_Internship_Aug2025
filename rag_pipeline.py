# rag_pipeline.py
import os
import chromadb
from sentence_transformers import SentenceTransformer
from openai import OpenAI

# -------------------------
# Configuration
# -------------------------
COLLECTION_NAME = "audiobook_chunks"
EMBEDDING_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
LLM_MODEL = "gpt-4o-mini"  # or "gpt-3.5-turbo"

# -------------------------
# Setup ChromaDB (Persistent)
# -------------------------
client = chromadb.PersistentClient(path="./chroma_db")

# Create collection if it does not exist
try:
    collection = client.get_collection(COLLECTION_NAME)
    print(f"✅ Collection '{COLLECTION_NAME}' exists, loaded.")
except Exception:
    collection = client.create_collection(COLLECTION_NAME)
    print(f"✅ Collection '{COLLECTION_NAME}' created.")

# -------------------------
# Setup Embedding & LLM
# -------------------------
embedder = SentenceTransformer(EMBEDDING_MODEL_NAME)

openai_client = OpenAI(api_key="")


# -------------------------
# Helper functions
# -------------------------
def trim_context(chunks, max_chars=7000):
    """Join chunks until max_chars limit is reached."""
    ctx, total = [], 0
    for c in chunks:
        if total + len(c) > max_chars:
            break
        ctx.append(c)
        total += len(c)
    return " ".join(ctx)

def build_prompt(query, context):
    """Build a prompt that instructs the LLM to use only the context."""
    return f"""
You are a helpful assistant. Use ONLY the context below to answer.
If the answer is not in the context, reply: "The book does not provide this information."

Context:
{context}

Question: {query}

Answer:
"""

# -------------------------
# Main RAG function
# -------------------------
def rag_pipeline(query, top_k=5, max_chars=5000):
    """
    Retrieve relevant chunks from ChromaDB and generate answer via LLM.
    Returns:
        answer: string generated by LLM
        citations: list of metadata of retrieved chunks
        docs: list of retrieved chunk texts
    """

    # --- Step 2: Embed the query ---
    q_emb = embedder.encode(query).tolist()

    # --- Step 3: Retrieve top-K chunks ---
    results = collection.query(
        query_embeddings=[q_emb],
        n_results=top_k,
        include=["documents", "metadatas", "distances"]
    )

    docs = results["documents"][0]
    metas = results["metadatas"][0]
    dists = results["distances"][0]

    # --- Step 4: Assemble context ---
    context = trim_context(docs, max_chars=max_chars)

    # --- Step 5: Build prompt ---
    prompt = build_prompt(query, context)

    # --- Step 6: Call LLM ---
    response = openai_client.chat.completions.create(
        model=LLM_MODEL,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.0
    )

    # --- Step 7: Get answer ---
    answer = response.choices[0].message.content

    # --- Prepare citations ---
    citations = [
        {
            "file_path": m.get("file_path", "unknown"),
            "chunk_index": m.get("chunk_index", i),
            "distance": float(dists[i])
        }
        for i, m in enumerate(metas)
    ]

    return answer, citations, docs

    

